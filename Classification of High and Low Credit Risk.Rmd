---
title: "ML with caret for classification problem of classifying good (low) and bad (high) credit risk"
author: "ABC"
date: "11/12/2022"
output: 
  html_document:
    code_folding: "show"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r Load libraries and data}
library(tidyverse)
library(caret)
library(e1071)
library(glmnet)
library(MLmetrics)
library(caretEnsemble)
library(kernlab)
data(GermanCredit)
```

 

Throughout this Project, we will cover:

* General preparation of the dataset for machine learning
* Visualization of feature distribution by class
* Pre-processing: imputation of missing data, one-hot encoding, and normalization
* Removing low information features
* Visualization of feature importance
* Definitions of metrics of performance for classification problems: Sensitivity, Specificity, etc.
* Hyperparameter tuning, using either a preset tuning length or a specified tuning grid
* Using non-standard sampling methods to correct for class imbalance
* Altering boundaries for classifier thresholds
* Training and resampling multiple models
  
There are a few sources from which this project draws influence and structure.   The first is the GitHub documentation on "caret" from its creation, Max Kuhn.   The second is a very well-written and comprehensive tutorial by author Selva Prabhakaran on Machine Learning Plus.   Third is a helpful resource for dealing with class imbalance, as we often find with classification problems.

- GitHub documentation from Max Kuhn: https://topepo.github.io/caret/
- Tutorial by Selva Prabhakaran: https://www.machinelearningplus.com/machine-learning/caret-package/
- Tutorial on "caret" with class imbalances: https://shiring.github.io/machine_learning/2017/04/02/unbalanced 

Here are some steps we're going to take to get this data into a form more realistic to what you may experience in the real world...

<br>

### General preparation of the dataset for machine learning ###

```{r Manipulation of data for required form}
# Select variables
GermanCredit <- GermanCredit %>%
  dplyr::select(Class, Duration, Amount, Age, ResidenceDuration, NumberExistingCredits,
                NumberPeopleMaintenance, Telephone, ForeignWorker, Housing.Rent,
                Housing.Own, Housing.ForFree, Property.RealEstate,
                Property.Insurance, Property.CarOther, Property.Unknown) %>%
  dplyr::rename("EmploymentDuration" = "Duration")

# Simulate missing data for the variables Age and Employment Duration
n <- nrow(GermanCredit)
agePct <- 3
durationPct <- 7
# Generate rows that will hold missing data
set.seed(355)
ageMissingPctRows <- sample(1:n, round(agePct/100 * n, 0))
set.seed(355)
durationMissingPctRows <- sample(1:n, round(durationPct/100 * n, 0))
# Make values NA's
GermanCredit[ageMissingPctRows, "Age"] <- NA
GermanCredit[durationMissingPctRows, "EmploymentDuration"] <- NA
# Code certain variables as factors
GermanCredit <- GermanCredit %>%
  mutate(across(.cols = c("ResidenceDuration", "NumberExistingCredits",
                          "NumberPeopleMaintenance", "Telephone",
                          "ForeignWorker"), .fns = factor))
```

Let's get a look at our dataset now:

```{r Summary of dataset}
summary(GermanCredit)
```

"Class" is our response variable, and it has a class balance of 70/30.    We now have a distribution of missing values for the EmploymentDuration and Age variables that we will address later, but the rest of our predictor variables are factors.    Notice that they are coded in different ways.  For example, "Telephone" and "ForeignWorker" are coded as 0 vs. 1 variables, but the variable "Housing" is divided into three components: "Housing.Rent", "Housing.Own", and "Housing.ForFree".   We will address this during the pre-processing process.

<br>

### Visualization of feature distribution by class ###

Caret gives us the very useful `featurePlot()` function, which can help produce lattice graphs - that is, to observe the distribution of the predictors by the class variable when we have continuous variables.  Let's look at a couple examples of possible feature plots.

```{r Feature plot using a boxplot}
featurePlot(x = GermanCredit[,c("EmploymentDuration", "Age")],
            y = GermanCredit$Class,
            plot = "box")
```

```{r Feature plot using a density plot with continuous variables}
featurePlot(x = GermanCredit[,c("EmploymentDuration", "Age")],
            y = GermanCredit$Class,
            plot = "density")
```

In the case of the "Property" variable which is coded numerically, we can do a similar procedure. 

```{r Feature plot using a density plot with categorical variables}
featurePlot(x = GermanCredit[,13:16],
            y = GermanCredit$Class,
            plot = "density")
```

Another very helpful function is `nearZeroVar()`, which can identify variables that have either one unique value (i.e. true "zero variance" predictors, or predictors that have very few unique values relative to the total sample size, or a very large ratio of frequency of the most common value to the next most common value).    Let's use it now with the default arguments as well as with a more extreme example:

```{r Example of nearZeroVar() function}
nearZeroVar(GermanCredit, freqCut = 95/5, uniqueCut = 10)
nearZeroVar(GermanCredit, freqCut = 80/20, uniqueCut = 10)
```

This function returns the column indices of the variables that are thought to be near zero variation, based on this configuration.   The variable being complained about in the first run of the function is the "ForeignWorker" variable.  Actually, you can see that under the lower configuration, it complains about variables 10, 12, and 16 - but these are levels of the "Housing" and "Property" variables.   Technically it should be complaining about the "NumberExistingCredits" variable, but doesn't due to the way it is encoded!

Either way, we will drop the variable "ForeignWorker" due to the relative lack of variation, and will merge levels 2, 3, and 4 of the variable NumberExistingCredits.

```{r Collapse one of the factors based on lack of variation}
GermanCredit <- dplyr::select(GermanCredit, -ForeignWorker)
GermanCredit$NumberExistingCredits <- fct_collapse(GermanCredit$NumberExistingCredits,
                                                    "2+" = c("2", "3", "4"))
```

<br>

### Pre-processing: imputation of missing data, one-hot encoding, and normalization ###

Let's move on to other pre-processing functions.  The first thing that we will do is divide our data into two parts: training set and test set.   Caret provides us the `createDataPartition()` function for this, which will allow us to partition based on the proportion from the response variable.

```{r Partition data into training and test sets}
set.seed(355)
trainIndex <- createDataPartition(GermanCredit$Class, p = 0.7, list = FALSE)
trainingSet <- GermanCredit[trainIndex,]
testSet <- GermanCredit[-trainIndex,]
```

Let's summarize the training set by itself.

```{r Summary of training set}
summary(trainingSet)
```

Next, we are going to pre-process the data.   Your best friend for this process will be the `preProcess()` function.   We will use this to impute missing data first. 

The `preProcess()` function takes argument "method", which has many different options for processing.   For imputation, options are "knnImpute", "bagImpute", or "medianImpute".   Let's use "bagImpute" on the training set.   We will go back at the end and apply the same transformation to the testing data.

```{r Use the bagging imputation method for filling in missing data}
set.seed(355)
bagMissing <- preProcess(trainingSet, method = "bagImpute")
trainingSet <- predict(bagMissing, newdata = trainingSet)
```

Next, we will use what is known as "one hot encoding" to transform the dummy variables.   Actually, "Housing" and "Property" are already in the exact format that we want!   What we want to do is transform the other variables into the same format.   The output will be a matrix of the predictors, which omits the response variable.

```{r Transform to a one-hot encoding data structure}
dummyModel <- dummyVars(Class ~ ., data = trainingSet)
trainingSetX <- as.data.frame(predict(dummyModel, newdata = trainingSet))
```

The next thing that we will do will be to transform these variables to be between 0 and 1.   One of my preferred approaches, in the case where all of the predictors are continuous, is to standardize the variables into Z-scores.   This can be done through a combination of "center" and "scale" to the "method" argument.   Specifying "method = 'range'", however, will transform the variables to a 0-1 scale.

```{r Normalize predictors}
rangeModel <- preProcess(trainingSetX, method = "range")
trainingSetX <- predict(rangeModel, newdata = trainingSetX)
```

Now, we will make the final training set by adding this to our original response variable.

```{r Add response variable back to training set}
trainingSet <- cbind(trainingSet$Class, trainingSetX)
names(trainingSet)[1] <- "Class"
```

But remember, all we did was transform the training set.   We need to transform the test set as well.   We'll use the same three procedures: the imputation of missing values using the "bagMissing" model object, the one-hot encoding using the "dummyModel" object, and the normalization using the "rangeModel" object.

```{r Apply the same transformations to the test set}
testSet_imputed <- predict(bagMissing, testSet)
testSet_dummy <- predict(dummyModel, testSet_imputed)
testSet_range <- predict(rangeModel, testSet_dummy)
# Output turns into matrix we need dataframes
testSet_range <- data.frame(testSet_range)
testSet <- cbind(testSet$Class, testSet_range)
names(testSet) <- names(trainingSet)   # proactive step that can prevent errors later
```

<br>

### Removing low information features ###

Next thing that we need to consider is low information features.   If uninformative, useless features are included in the dataset, this will almost always lead to a decrease in model performance.   Personally, I like to just let the domain knowledge take care of this part.   However, one other option is Recursive Feature Elimination.   The function to implement this is the `rfe()` function, with a control defined by the `rfeControl()` function.

Recursive Feature Elimination works by building many models of a type of machine learning method on the training set, and iteratively re-calculating the most important variables.   At the end, the variables that were found important most often, across different subset sizes, can be reported.   We will see a single example of a variable importance plot later; many algorithms provide methods for ranking features from most to least important.   The method that will be used in our Recursive Feature Elimination approach here will be the Random Forest, and subset sizes (i.e. number of most important features to use) explicitly provided.

```{r Example of recursive feature elimination}
subsets <- c(1:5, 10, 15, 20)
set.seed(355)
rfeCtrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      verbose = FALSE)
rfProfile <- rfe(x = trainingSet[,2:21], 
                y = trainingSet$Class, 
                sizes = subsets,
                rfeControl = rfeCtrl)
rfProfile
```


We will not be eliminating features in this example based on this; however, this is something that is well worth being aware of!

<br>

### Visualization of feature importance ###

Next, we will be using the `train()` function to fit actual models, and later going in to examine performance.   The `train()` function is an incredibly powerful function, that takes a control object which controls for tuning hyperparameters, cross-validation of the model, and selecting an optimal model.

To get an idea of the full scope of models that can be trained in "caret", see the following list:

```{r See the list of available ML methods}
names(getModelInfo())
```

As you can see, there are MANY options.   However, it should be noted that certain packages, on top of "caret", must be installed and called at the beginning in order to use these.   We will start by training a Random Forest classifier on our training set and looking at what we can do with it.   Note that we are not setting the controls for the `train()` function - it will do this automatically.   We will come back later and do so.

```{r Train a random forest}
set.seed(355)
rf <- train(Class ~., data = trainingSet, method = "rf")
rf
```

Random Forest is one of several different classifiers that provides a metric of variable importance.  Some others include linear models (where the absolute value of the t-statistic for each model parameter is used to rank variables by importance), partial least squares, recursive partitioning, bagged or boosted trees, or multivariate adaptive regression splines.   As a next step, we will look at a plot of the variable importance for the Random Forest we just trained.

```{r Variable importance plot from random forest}
varimp_RF <- varImp(rf)
plot(varimp_RF, main = "German Credit Variable Importance (Random Forest)")
```

Some caution should be taken in interpreting these Random Forest variable importance plot results, because they tend to weight continuous variables higher than categorical variables.  See my YouTube video on "When Should You Use Random Forests?" to see a more detailed example of this.  It is fair however to note that Amount is viewed as a very disproportionately important feature.

<br>

### Definitions of metrics of performance for classification problems: Sensitivity, Specificity, etc. ###

Let's now use the random forest to predict on our test data.   The `predict()` function here, by default, will return response predictions (i.e. a vector of "Good"s and "Bad"s).  We will use it later to output probabilities instead.

```{r Create fitted values (classification decisions)}
fitted <- predict(rf, testSet)
fitted[1:10]
```

Now we will create a confusion matrix:

```{r Confusion matrix comparing fitted values to actual values}
confusionMatrix(reference = testSet$Class, data = fitted, mode = "everything", positive = "Good")
```

It is worth taking a moment to interpret some of this output.

* Sensitivity: Of the number of times a credit class was "good", how often did the model correctly identify these?   This term is also known as "recall".
* Specificity: Of the number of times a credit class was "bad", how often did the model correctly identify these?
* Pos Pred Value: The positive predictive value represents, out of how often the model identified subjects as having "good" credit class, how many truly had a "good" credit class.   This is also known as "precision".

As you can see here, the model has reasonable sensitivity and positive predictive value, but abysmal specificity.    Such is life for practitioners of machine learning.   

But next, we need to actually tune the hyperparameters here rather than just pass these directly into the `train()` function!

The `train()` function allows us to pass in a control to it.   Part of this is the hyperparameter tuning.   There are two ways to do this: through a tuning length, or through a tuning grid.    The tuning length, specified through the "tuneLength" argument, represents the number of unique values that the "train" function will automatically choose between as the model trains.   "tuneGrid" allows the user to specify which values for the hyperparameters that they want.

```{r Define a training control}
twoClassCtrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5,
  savePredictions = "final",
  classProbs = T,
  summaryFunction = twoClassSummary # Mean we want to optimize across the tradeff between sensitivity and specifity
)
```

The "summaryFunction" argument to trCtrl is an important one.    One can specify whether they would like to optimize across the tradeoff between Sensitivity and Specificity by specifying the "twoClassSummary" as the summary function.   Alternatively, one can optimize across the tradeoff between Recall and Precision (that is, Sensitivity and Positive Predictive Value) by specifying "prSummary" as the summary function.   Your knowledge of the domain should be the guiding force behind this determination.   However, as a general rule, for very imbalanced datasets, Precision/Recall is a preferable tradeoff than Sensitivity/Specificity.   This particular case is borderline because the class imbalance is 70/30 - it's imbalanced but you'll see a lot worse in the real world.   We will hold it in place for now.

Now that we've created a training control, we'll also use the "tuneLength" argument to the `train()` function.

```{r Use the training control with a pre-specified tuning length}
set.seed(355)
rfTL <- train(Class ~., data = trainingSet, method = "rf", metric = "ROC", trControl = twoClassCtrl, tuneLength = 10) # Wen want to try for 10 different values of mtry hyperparametr and pick best out of these 10 values unlike 2,11, 20 as above
fittedTL <- predict(rfTL, testSet)
confusionMatrix(reference = testSet$Class, data = fittedTL, mode = "everything", positive = "Good")
```

This is a little bit more balanced, but not exactly a huge improvement.    Next, let's make changes using tuneGrid.   Before doing this, it may be a helpful to look at what parameters we are actually tuning.

```{r Look up hyperparameters of random forest}
modelLookup("rf")
```

There is one parameter here to tune, and that is "mtry", or the number of randomly selected predictors in the tree.   Let's take this for a spin...

```{r Use the tuning grid approach for model tuning}
rfGrid <- data.frame(mtry = c(3, 5, 7, 9, 10, 11, 12, 13, 15, 17, 19))  # tuneGrid requires a data frame input
set.seed(355)
rfTG <- train(Class ~., data = trainingSet, method = "rf", metric = "ROC", trControl = twoClassCtrl, tuneGrid = rfGrid) # tuneGrid for randomly trying values of mtry
fittedTG <- predict(rfTG, testSet)
confusionMatrix(reference = testSet$Class, data = fittedTG, mode = "everything", positive = "Good")
```

<br>

### Using non-standard sampling methods to correct for class imbalance ###

Next, another consideration we must have is sampling method.   The two primary techniques for doing this are "down-sampling" and "up-sampling", although "caret" does offer a couple hybrid approaches.   These approaches are called "SMOTE" and "ROSE", and require installation of the "DMwR" and "ROSE" packages, respectively.

* down-sampling: In this approach, we purposely under-sample the majority class.    In the example here where 70% of the rows are of credit risk "Good" and 30% are of credit risk "Bad", we sample from the training set such that the two classes are of the same frequency (in effect, we would use only 60% of the training set).

* up-sampling: In this approach, we would over-sample the minority class such that we have an equal number of rows from the two classes.

The method for sampling can be specified in the control, so let's try that next.

```{r Define controls for down or up-sampling}
downCtrl <- trainControl(
  method = "boot",
  number = 5,
  savePredictions = "final",
  classProbs = T,
  summaryFunction = twoClassSummary,
  sampling = "down"
)
upCtrl <- trainControl(
  method = "boot",
  number = 5,
  savePredictions = "final",
  classProbs = T,
  summaryFunction = twoClassSummary,
  sampling = "up"
)
```

Now we'll try looking at model performance again (for the down-sampled training case):

```{r Use down-sampling control and see performance}
set.seed(355)
rfDown <- train(Class ~., data = trainingSet, method = "rf", metric = "ROC", trControl = downCtrl, tuneLength = 10)
fittedDown <- predict(rfDown, testSet)
confusionMatrix(reference = testSet$Class, data = fittedDown, mode = "everything", positive = "Good")
```

Then let's look at the model performance for the up-sampled training case:

```{r Use up-sampling control and see performance}
set.seed(355)
rfUp <- train(Class ~., data = trainingSet, method = "rf", metric = "ROC", trControl = upCtrl, tuneLength = 10)
fittedUp <- predict(rfUp, testSet)
confusionMatrix(reference = testSet$Class, data = fittedUp, mode = "everything", positive = "Good")
```

<br>

### Altering boundaries for classifier thresholds ###

An entirely out of the box approach for attacking the class imbalance problem is, when we use the `predict()` function, to change these to probabilities.   For example, if the algorithm assigns a probability of 33% or greater that an observation will fall into the negative class, we can classify it as Bad.

```{r Create new thresholds for classification}
fittedProb <- predict(rfTL, testSet, type = "prob")
fittedProb <- fittedProb$Bad
fittedProb <- factor(ifelse(fittedProb >= 0.333, "Bad", "Good"))
```

Now we will create a confusion matrix:

```{r View performance using new threshold}
confusionMatrix(reference = testSet$Class, data = fittedProb, mode = "everything", positive = "Good")
```

While this is not the most accurate approach that has been used so far, it is by far the most balanced with respect to Sensitivity and Specificity.   This can be thought of somewhat analogously to treating the misclassification of bad creditors as good, to be twice as serious an error as misclassifying good creditors as bad; and in fact, the two errors not being equally bad will often be true when there is class imbalance.   Note that this assessment (i.e. one error being 2X or 3X as bad as the other kind) should ALWAYS be informed primarily by knowledge of the underlying domain.

<br>

### Training and resampling multiple models ###

Our next order of business will be to evaluate model performance across many different methods, instead of sticking strictly to just the Random Forest!    We will compare the Random Forest to two other approaches: "glmnet" (the Elastic Net), and "svmRadial" (Support Vector Machines with a radial kernel).   We will define a list of methods we want to use, and create an ensemble of training results using the `caretList()` functionality from the "caretEnsemble" package.

Specify the training protocol:

```{r Define control for training of multiple methods}
methodCtrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5,
  savePredictions = "final",
  classProbs = T,
  summaryFunction = twoClassSummary
)
```

Specify list of methods:

```{r Train the Random Forest}
methodList <- c("rf", "glmnet", "svmRadial")
```

Train the ensemble of models:

```{r Train model ensemble}
set.seed(355)
ensemble <- caretList(Class ~ ., data = trainingSet, metric = "ROC", trControl = methodCtrl, methodList = methodList)
```

Now we will compare model performance using the `resamples()` function:

```{r Compile various model performances}
resampledList <- resamples(ensemble)
summary(resampledList)
```

Because the Random Forest is outperforming the other methods, from the standpoint of ROC, it is probably the preferred approach of these three.
 
